{"cells":[{"cell_type":"markdown","source":["# Interactive Analytics\n<br>\n## Task\nRun interactive queries on the data and answer some analytical questions\n\n## Data\n* use two datasets about influencers\n* the first dataset contains basic information about each influencer\n* the second dataset contains posting history for each influncer for the past 6 months\n\n## Questions:\n1) Count number of distinct languages used in the dataset\n\n2) List all interests in descending order depending on its frequency\n\n3) How many influencers interested in Technology using english language (en) published a post in july and did not publish a post in june\n\n4) For each influencer add a new column n_posts with number of posts he published in its history \n\n5) Find top 10 influencers that posted the most posts in their history\n\n6) Compute entropy of the n_posts column for each interest"],"metadata":{}},{"cell_type":"markdown","source":["## Documentation\n<br>\n* Pyspark documentation of DataFrame API is <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\">here</a>\n\n* Prezentation slides are accessed <a target=\"_blank\" href = \"https://docs.google.com/presentation/d/1XNKIfE5Atj_Mzse0wjmbwLecmVs2YkWm9cqOLqDVWPo/edit?usp=sharing\">here</a>"],"metadata":{}},{"cell_type":"markdown","source":["### Import functions and modules"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import count, col, size, greatest, collect_list, when, explode, sum, rank, dense_rank, desc, row_number, avg, pandas_udf, PandasUDFType, array_contains, array_sort, lit, dayofweek\nfrom pyspark.sql import Window\nfrom pyspark.sql.types import DateType, DoubleType, StructType, StructField, DecimalType, LongType, StringType\n\nimport scipy.stats"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Load the data"],"metadata":{}},{"cell_type":"code","source":["infls = spark.table('mlprague.influencers')\n\nposts_history = spark.table('mlprague.infl_posting_history')"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Explore the data\n\nhint\n* check the schema call df.printSchema()\n* check the row count - call df.count()\n* see some rows - call df.show(), or display(df)"],"metadata":{}},{"cell_type":"code","source":["infls.printSchema()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(infls)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["infls.count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Count number of distinct languages used in the dataset (*)\n\nhint\n* use <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode\">explode</a> and <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct\">distinct</a>"],"metadata":{}},{"cell_type":"code","source":["(\n  infls\n  .withColumn('language', explode('languages'))\n  .select('language')\n  .distinct()\n  .count()\n)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### List all interests in descending order depending on its frequency (\\* \\*)\n\nhint:\n* use explode on the array\n* use groupBy with some aggregation function (slide 25 in the presentation might be useful for aggregations)\n* use <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy\">orderBy</a>"],"metadata":{}},{"cell_type":"code","source":["display(\n  infls\n  .withColumn('interest', explode('interests'))\n  .groupBy('interest')\n  .agg(\n    count('*').alias('interest_frequency')\n  )\n  .orderBy(desc('interest_frequency'))\n)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### How many influencers interested in Technology using english language (en) published a post in july and did not publish a post in june (\\* \\* \\*)\n\nhint\n* Possible way how to do it:\n * join influencers with their posting history\n * create 2 dataframes - in the first one select influencers that published in july, in the second select influencers that published in june\n * apply filters for interest and language\n * do a left anti join ('left_anti')\n * deduplicate the result\n* <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join\">joins</a> in the docs\n* slide 20 in the presentation might be useful for joins\n* <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropDuplicates\">dropDuplicates</a> in the docs"],"metadata":{}},{"cell_type":"code","source":["july = (\n  infls\n  .join(posts_history, 'influencer_id')\n  .filter(array_contains(col('languages'), 'en'))\n  .filter(array_contains(col('interests'), 'Technology'))\n  .filter(col('post_date').between('2018-07-01', '2018-07-31'))\n)\n\njune = (\n  infls\n  .join(posts_history, 'influencer_id')\n  .filter(array_contains(col('languages'), 'en'))\n  .filter(array_contains(col('interests'), 'Technology'))\n  .filter(col('post_date').between('2018-06-01', '2018-06-30'))\n)\n\njuly.join(june, 'influencer_id', 'left_anti').dropDuplicates(['influencer_id']).count()\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### For each influencer add a new column n_posts with number of posts he published in its history (\\* \\*)\n\nhint\n* This question is prerequisite for the two next questions\n* Possible way how to do it:\n * aggregate the posting history and join the result to influencers"],"metadata":{}},{"cell_type":"code","source":["n_posts = (\n  posts_history\n  .groupBy('influencer_id')\n  .agg(\n    count('*').alias('n_posts')\n  )\n)\n\ninfls_with_n_posts = (\n  infls\n  .join(n_posts, 'influencer_id')\n  \n)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(infls_with_n_posts)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Find top 10 influencers that posted the most posts in their history (*)\n\nhint\n* this question depends on the result from the previous question\n* orderBy the previous result"],"metadata":{}},{"cell_type":"code","source":["display(\n  infls_with_n_posts\n  .orderBy(desc('n_posts'))\n  .limit(10)\n)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Compute entropy of the n_posts column for each interest (\\* \\* \\* \\* \\*)\n\nhint\n* This question depends on results from the previous 2 questions\n* There are at least 2 ways how you can solve this (choose which way you prefer)\n* Way 1:\n * Using higer order functions (slide 55 in the presentation might be useful)\n * first compute probability for each interest (for each interest get an array with probabilities of n_posts col)\n * (you might want to groupBy interest and n_posts than use a window and finaly groupBy again by interest and use collect_list for final aggregation)\n * use AGGREGATE in SQL expression to compute the entropy from the array\n* Way 2:\n  * Using Pandas grouped_map UDF (slide 40 in the presentation might be useful)\n  * Define schema for the udf\n  * Implement the udf (to compute the probabilities you can use value_counts() of Pandas.Series and divide by number of rows using len(pandas dataframe))\n  * Use scipy.stats.entropy for the entropy calculation\n  \nNote\n* For entropy calculation use the same definition as is used in scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n\n$$S = - \\Sigma_{i} \\left(p\\left(i\\right) * \\log\\left(p\\left(i\\right)\\right)\\right)$$\n\n* for probability computation you can use this approach: compute the frequency of each value and divide by the total number of values\n * example: suppose we have this array [10, 10, 11, 12], the probability of 10 is p(10) = 2/4 = 0.5, the probability of 11 is p(11) = 1/4 = 0.25"],"metadata":{}},{"cell_type":"code","source":["# Using Higher order functions:\n\ninterests_with_probability = (\n  infls_with_n_posts\n  .select('n_posts', explode('interests').alias('interest'))\n  .select(col('n_posts').cast(DoubleType()).alias('n_posts'), 'interest')\n  .groupBy('interest', 'n_posts')\n  .agg(count('*').alias('frequency'))\n  .withColumn('n_posts_total', sum('frequency').over(Window.partitionBy('interest')))\n  .withColumn('n_posts_probability', col('frequency') / col('n_posts_total'))\n  .groupBy('interest')\n  .agg(\n    collect_list(col('n_posts_probability')).alias('n_posts')\n  )\n)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["display(interests_with_probability)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["result = interests_with_probability.selectExpr(\n  'interest',\n  'AGGREGATE(n_posts, CAST(0 AS double), (buffer, value) -> (buffer - value * Log(value))) AS n_posts_entropy'\n).orderBy('n_posts_entropy')"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["display(result)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# using Pandas GROUPED_MAP UDF:\n\ninterests_with_n_posts = (\n  infls_with_n_posts\n  .select('n_posts', explode('interests').alias('interest'))\n)\n\n\nschema = StructType(\n  [\n    StructField('n_posts', LongType()),\n    StructField('interest', StringType()),\n    StructField('entropy', DoubleType())\n  ]\n)\n\n@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\ndef udf_entropy(pdf):\n    p_data= pdf['n_posts'].value_counts() / len(pdf) # calculates the probabilities\n    pdf['entropy'] = scipy.stats.entropy(p_data)\n    return pdf"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["result = (\n  interests_with_n_posts\n  .groupBy('interest')\n  .apply(udf_entropy)\n  .select('interest', 'entropy')\n  .dropDuplicates()\n  .orderBy('entropy')\n)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["display(result)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### You may want to take a look at the distribution for given interests"],"metadata":{}},{"cell_type":"code","source":["display(\n  interests_with_n_posts\n  .filter(col('interest') == 'Technology')\n  .groupBy('n_posts')\n  .agg(\n    count('*')\n  )\n  .orderBy('n_posts')\n)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["display(\n  interests_with_n_posts\n  .filter(col('interest') == 'Hobbies and activities')\n  .groupBy('n_posts')\n  .agg(\n    count('*')\n  )\n  .orderBy('n_posts')\n)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":33}],"metadata":{"name":"interactive_analytics","notebookId":1865},"nbformat":4,"nbformat_minor":0}
