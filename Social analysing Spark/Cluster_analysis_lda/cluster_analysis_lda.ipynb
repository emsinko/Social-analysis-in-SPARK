{"cells":[{"cell_type":"markdown","source":["# Cluster Analysis and topic modelling using LDA\n\n## Task\nCluster the posts using LDA (Latent Dirichlet Allocation)\n\n## Data\n* Take the same data that was used with KMeans - posts on facebook pages, but take only the cluster that corresponds to english pages\n\n## Notes\n* Use LDA instead of KMeans\n* You may want to play with number of topics and the size of vocabulary (the default size of CountVectorizer is 262144)\n* You may want to do some more preprocessing of the text\n * for instance remove punctuation\n * or add some more words on the list provided to the StopWordsRemover\n\n\n## About LDA\n* for more details about LDA see <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">wiki</a>\n* LDA model assumes that each document (post message in our case) is composed of some topics (number of these topics has to specified as input parameter)\n* Each of these topics can be characterized by a set of words (bellow we provide a udf get_words that allows you to see the words to each topic)\n* For each document you will get a topic distribution (a probability or weight for each topic in the document)\n* The most probable topic in the document can be interpreted as cluster (bellow we provide a udf get_cluster that gives you index of the most probable topic)"],"metadata":{}},{"cell_type":"markdown","source":["## Documentation\n<br>\n* Pyspark documentation of DataFrame API is <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\">here</a>\n\n* Pyspark documentation of ML Pipelines library is <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html\">here</a>\n\n* Prezentation slides are accessed <a target=\"_blank\" href = \"https://docs.google.com/presentation/d/1XNKIfE5Atj_Mzse0wjmbwLecmVs2YkWm9cqOLqDVWPo/edit?usp=sharing\">here</a>"],"metadata":{}},{"cell_type":"markdown","source":["### Import functions and modules"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, count, desc, array_contains, split, explode, regexp_replace, lit\n\nfrom pyspark.sql.types import ArrayType, StringType\n\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer, CountVectorizer\n\nfrom pyspark.ml.clustering import LDA\n\nfrom pyspark.ml import Pipeline\n\n\nimport numpy as np"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Load Data\n\nhint\n* here we will use the dataset that you saved in the previous notebook so copy the table_name and use it here"],"metadata":{}},{"cell_type":"code","source":["# take the generated name from the previous notebook:\ntable_name = 'muutodfmuwfcmpxjfvwy'\n\ndata = spark.table(table_name)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Explore the data\n\nhint\n* see how many records you have"],"metadata":{}},{"cell_type":"code","source":["data.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(data)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Remove punctuation\n\nhint\n* it seems to be reasonable to do some more preprocessing on the data - one of the steps is removing the punctuation\n* you can use <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.regexp_replace\">regexp_replace</a> function of DF API\n* you may try to use this (or some similar) regular expression: \"[(.|?|,|:|;|!|>|<)]\""],"metadata":{}},{"cell_type":"code","source":["reg = \"[(.|?|,|:|;|!|>|<)]\"\n\npages = data.withColumn('message', regexp_replace('message', reg, ' '))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### See how many words you have in total in your documents\n\nhint\n* use functions <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.split\">split</a> and <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode\">explode</a> on the message field\n* select the exploded message field and call <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct\">distinct</a> on it (or use <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropDuplicates\">dropDuplicates</a> equivalently)\n* count number of rows"],"metadata":{}},{"cell_type":"code","source":["(\n  pages\n  .withColumn('words', split('message', ' '))\n  .select(explode('words').alias('word'))\n  .distinct()\n  .count()\n)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Construct the pipeline\n\nhint\n* do vector representation for the texts\n * use: \n * <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Tokenizer\">Tokenizer</a> \n * <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover\">StopWordsRemover</a> \n * <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.CountVectorizer\">CountVectorizer</a>\n * <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.IDF\">IDF</a> \n * <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Normalizer\">Normalizer</a> \n * <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.LDA\">LDA</a>\n* you will have to choose number of topics for the LDA\n* See the slides 83, 84, 85, 101 in the presentation\n\nNotes\n* with KMeans we used HashingTF to compute the term frequency as input for IDF\n* here we are using countVectorizer so we can work with actual words and see how the topics are described later on"],"metadata":{}},{"cell_type":"code","source":["tokenizer = Tokenizer(inputCol='message', outputCol='words')\n\nstopWordsRemover = StopWordsRemover(inputCol='words', outputCol='noStopWords')\n\ncountVectorizer = CountVectorizer(vocabSize=1000, inputCol='noStopWords', outputCol='tf', minDF=1)\n\nidf = IDF(inputCol='tf', outputCol='idf')\n\nnormalizer = Normalizer(inputCol='idf', outputCol='features')\n\nlda = LDA(k=7, maxIter=10)\n\npipeline = Pipeline(stages=[tokenizer, stopWordsRemover, countVectorizer, idf, normalizer, lda])\n\nmodel = pipeline.fit(pages)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Apply the model on the data\n\nhint\n* just call transform, since the model is a transformer\n* pass the training data as argument to the transform function"],"metadata":{}},{"cell_type":"code","source":["predictions = model.transform(pages)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## See the result of LDA\n\nhint\n* select name, message, topicDistribution to see the probabilities for each topic in given document"],"metadata":{}},{"cell_type":"code","source":["display(\n  predictions\n  .select('message', 'topicDistribution')\n)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Helper functions (udfs)"],"metadata":{}},{"cell_type":"code","source":["# Some useful UDFs that will help you to do the next tasks\n\n# vocabulary your model is using:\nvocab = model.stages[2].vocabulary\n\n# udf to extract the words for the topics\n@udf(ArrayType(StringType()))\ndef get_words(termIndices):\n  return [vocab[idx] for idx in termIndices]\n\n\n# udf to determine the main topic for the document\n@udf('integer')\ndef get_cluster(vec):\n  return int(np.argmax(vec))\n\n\n# udf to get the probability of a given topic in the document\n@udf('double')\ndef get_topic_probability(vec, topic):\n  return float(vec[topic])"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Describe topics\n\nhint\n* each topic is characterized by a set of words\n* use <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.LDAModel.describeTopics\">describeTopics()</a> method of the LDA model to get the indices of the words in your vocabulary (model.stages[n].describeTopics(), here n is the index of LDA in your pipeline)\n* use the udf get_words to see the actual words"],"metadata":{}},{"cell_type":"code","source":["display(\n  model.stages[5].describeTopics()\n  .withColumn('x', get_words(col('termIndices')))\n)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Find the most likely topic for each document\n\nhint\n* add new column named 'cluster' using the udf get_cluster to get the most likely topic for each post\n* as argument for the udf use column topicDistribution which the result of LDA. This column contains vector with probabilities for each topic in the post\n* you can now groupBy this new column and count how many posts are in given cluster"],"metadata":{}},{"cell_type":"code","source":["display(\n   predictions\n  .select('page_id', 'topicDistribution', 'message')\n  .withColumn('cluster', get_cluster('topicDistribution'))\n  .groupBy('cluster')\n  .count()\n)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Order the documents by probability of specific topic\n\nhint\n* choose a topic index (for example 0)\n* add new column called 'topicProbability' and extract here the probability your selected topic\n * these probabilities are in the column topicDistribution\n * to extract the probability you can use udf get_topic_probability implemented above. Just pass in the column topicDistribution and the index of your selected topic (you have to use the lit function for the topic index, for example: lit(0))\n* order the DataFrame in descending order by this new column topicProbability"],"metadata":{}},{"cell_type":"code","source":["display(\n   predictions\n  .select('page_id', 'topicDistribution', 'message')\n  .withColumn('topicProbability', get_topic_probability(col('topicDistribution'), lit(0)))\n  .orderBy(desc('topicProbability'))\n)"],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"cluster_analysis_lda","notebookId":1980},"nbformat":4,"nbformat_minor":0}
