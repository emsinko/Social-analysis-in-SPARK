{"cells":[{"cell_type":"markdown","source":["# Create predictive model\n<br>\n## Task\nConstruct model that is going to predict if an influencer is going to publish a post next day or not. Model it as binary classification.\n\n## Data\n* use two datasets about influencers\n* the first dataset contains basic information about each influencer\n* the second dataset contains posting history for each influncer for the past 6 months\n\n## Notes\n* the posting history is for the period 1.1.2018 - 1.8.2018\n* assume it is 31.7.2017 and make a prediction for the next day\n* extract the labels for 1.8. to constract the training and test dataset\n* extract some features from the available data\n* experiment with these models: Logistic regression (lr), decision tree (dt), random forest (rf)\n* Try to construct some basic model first and than improve it by adding some more features"],"metadata":{}},{"cell_type":"markdown","source":["## Documentation\n<br>\n* Pyspark documentation of DataFrame API is <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\">here</a>\n\n* Pyspark documentation of ML Pipelines library is <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html\">here</a>\n\n* Prezentation slides are accessed <a target=\"_blank\" href = \"https://docs.google.com/presentation/d/1XNKIfE5Atj_Mzse0wjmbwLecmVs2YkWm9cqOLqDVWPo/edit?usp=sharing\">here</a>"],"metadata":{}},{"cell_type":"markdown","source":["### Import functions and modules"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, max, datediff, count, desc, array_contains, broadcast, explode, length, first, when, expr, regexp_replace, row_number, coalesce, lit, coalesce, size\n\nfrom pyspark.sql import Window\n\n\nfrom pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder\nfrom pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Load Data"],"metadata":{}},{"cell_type":"code","source":["infl = spark.table('mlprague.influencers')\n\nposts_history = spark.table('mlprague.infl_posting_history')"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### You may want to do some exploratory analytics first\n\nhint:\n* see how many records you have\n* what is the schema of the dataset\n* see some records\n* use can use printSchema(), show(), count(), or proprietaray function display()"],"metadata":{}},{"cell_type":"code","source":["# your code here:\ninfl.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["posts_history.count()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["infl.printSchema()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["posts_history.printSchema()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["display(infl)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(posts_history)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Extract the label\n\nhint:\n* use the posts history dataset and see what influencers posted on 1.8.2018 and assign them label 1\n * use withColumn() transormation together with lit(1) which adds a column with constant value 1\n * see lit() function in <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.lit\">docs</a> with example\n* left join this on the influencers and those records with null value will have label 0"],"metadata":{}},{"cell_type":"code","source":["label = (\n  posts_history\n  .filter(col('post_date') == '2018-08-01')\n  .select('influencer_id')\n  .distinct()\n  .withColumn('label', lit(1))\n)\n\ninfluencers_with_label = (\n  infl\n  .join(label, 'influencer_id', 'left')\n  .withColumn('label', coalesce('label', lit(0)))\n)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### You may also want to check how many datapoints you have for each class\n\nhint\n* use groupBy('label').count()"],"metadata":{}},{"cell_type":"code","source":["display(\n  influencers_with_label\n  .groupBy('label')\n  .agg(count('*').alias('ct'))\n)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display(influencers_with_label)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Construct some basic features\n\nhint:\n* you may try number of interests, number of languages, age\n* interests and language cols are of ArrayType\n * you can use <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.size\">size</a> function to count number of its elements\n * the slide 48 in the prezentation might be useful for using functions on arrays"],"metadata":{}},{"cell_type":"code","source":["data_with_basic_features = (\n  influencers_with_label\n  .withColumn('num_interests', size('interests'))\n  .withColumn('num_languages', size('languages'))\n)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(data_with_basic_features)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Split the data for training and testing\n\nhint\n* use the function <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit\">randomSplit</a>\n* see the slide 99 in the presentation"],"metadata":{}},{"cell_type":"code","source":["(train, test) = data_with_basic_features.randomSplit([0.7, 0.3], 24)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Construct & fit the pipeline\n\nhint:\n* use <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler\">VectorAssembler</a> to create the input features \n* choose your model \n * for LR use <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier\">RandomForestClassifier</a> \n * for RF use <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression\">LogisticRegression</a> \n * for DT use <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier\">DecisionTreeClassifier</a> \n* the slide 104 in the prezentation might be useful for constructing the pipeline\n* use train data for training"],"metadata":{}},{"cell_type":"code","source":["# features:\nfeatures_array = ['num_interests', 'num_languages']\n\n# Assambler:\nassembler = VectorAssembler(inputCols=(features_array), outputCol='features')\n\n# Classifier:\nrf = RandomForestClassifier(labelCol='label', featuresCol='features', seed=42)\n\npipeline = Pipeline(stages=[assembler, rf])\n\nrf_model = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Evaluate the model\n\nhint: \n* use the <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.BinaryClassificationEvaluator\">BinaryClassificationEvaluator</a> \n* the slide 106 in the prezentation might be useful for evaluating binary classification\n* use the test data for evaluation"],"metadata":{}},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n\npredictions = rf_model.transform(test)\n\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display(predictions)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["The accuracy is not very great. Perhaps we can improve it by some more predictors"],"metadata":{}},{"cell_type":"markdown","source":["### Try to improve the model\n\nhint:\n* you may try also some categorical features like the value of the interest\n* the slide 88, 94 in the prezentation might be useful for OneHotEncoder and StringIndexer"],"metadata":{}},{"cell_type":"code","source":["data_with_catagorical_feature = (\n  data_with_basic_features.withColumn('interest', col('interests')[0])\n)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["display(data_with_catagorical_feature)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["(train, test) = data_with_catagorical_feature.randomSplit([0.7, 0.3], 24)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# features:\n\nfeatures_array = ['num_interests', 'num_languages']\n\n# indexer\ninterestIndexer = StringIndexer(inputCol='interest', outputCol='indexedInterest')\n\n# OneHotEncoders:\ninterestEncoder = OneHotEncoder(inputCol='indexedInterest', outputCol='interestVec')\n\n# Assambler:\nassembler = VectorAssembler(inputCols=(features_array + ['interestVec']), outputCol='features')\n\n# Classifier:\nrf = RandomForestClassifier(featuresCol='features', seed=42)\n\npipeline = Pipeline(stages=[interestIndexer, interestEncoder, assembler, rf])\n\nrf_model = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n\npredictions = rf_model.transform(test)\n\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["The accuracy is slightly better but still not very good. Let's see if we can improve it even better:"],"metadata":{}},{"cell_type":"markdown","source":["### Improve the model even more\n\nhint:\n* construct some features that capture how frequently the influencer posts\n* extract these features from the posting history"],"metadata":{}},{"cell_type":"code","source":["history_for_features = (\n  posts_history\n  .filter(col('post_date') <= '2018-07-31')\n)\n\ntime_from_last_post = (\n  history_for_features\n  .groupBy('influencer_id')\n  .agg(\n    max('post_date').alias('last_post')\n  )\n  .withColumn('time_from_last_post', datediff(lit('2018-07-31'), col('last_post')))\n  .select('influencer_id', 'time_from_last_post')\n)\n\nnumber_of_posts = (\n  history_for_features\n  .groupBy('influencer_id')\n  .agg(\n    count('*').alias('number_of_posts')\n  )\n)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["data_features_improved = (\n  data_with_catagorical_feature\n  .join(time_from_last_post, 'influencer_id')\n  .join(number_of_posts, 'influencer_id')\n)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["display(data_features_improved)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["(train, test) = data_features_improved.randomSplit([0.7, 0.3], 24)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# features:\nfeatures_array = ['num_interests', 'num_languages', 'time_from_last_post', 'number_of_posts']\n\n# indexer\ninterestIndexer = StringIndexer(inputCol='interest', outputCol='indexedInterest')\n\n# OneHotEncoders:\ninterestEncoder = OneHotEncoder(inputCol='indexedInterest', outputCol='interestVec')\n\n# Assambler:\nassembler = VectorAssembler(inputCols=(features_array), outputCol='features')\n\n# Classifier:\nrf = RandomForestClassifier(featuresCol='features', seed=42)\n\npipeline = Pipeline(stages=[interestIndexer, interestEncoder, assembler, rf])\n\nrf_model = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n\npredictions = rf_model.transform(test)\n\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["### Try crossvalidation\n\nhint\n* the slide 108 in the prezentation might be useful for tunning hyperparameters\n* check in the documentation what parameters has your model (maxDepth, numTrees for Random Forrest)"],"metadata":{}},{"cell_type":"code","source":["paramGrid = (\n  ParamGridBuilder()\n  .addGrid(rf.maxDepth, [3, 5, 8])\n  .addGrid(rf.numTrees, [50, 100, 150])\n  .build()\n)\n\ncross_model = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=paramGrid).fit(train)\n\nrf_model = cross_model.bestModel"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["predictions = rf_model.transform(test)\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["## See some properties of the final model\n\nNote\n* This depends on the model you are using\n\nHint\n* For Random Forest see the API of the model in <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassificationModel\">docs</a>\n* Example to see number of trees:\n * rf_model.stages[n].getNumTrees and here rf_model is your trained model and n is index of RF in your pipeline"],"metadata":{}},{"cell_type":"code","source":["rf_model.stages[3].getNumTrees"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["rf_model.stages[3].totalNumNodes"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["rf_model.stages[3].trees"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["rf_model.stages[3].toDebugString"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":52}],"metadata":{"name":"predictive_analytics","notebookId":1899},"nbformat":4,"nbformat_minor":0}
